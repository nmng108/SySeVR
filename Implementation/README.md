## Environment

> joern 0.3.1（jdk 1.7）, neo4j 2.1.5, python 3.5, tensorflow 1.6, gensim 3.4, joern-tools, xlrd, imbalanced-learn 0.6.2

## Phase 1: Generating slices (i.e., SeVCs)

Step 1-6 should follow detail_instructions.md

1. Use joern to parse source code: the input is source code files, and the output is a file named .joernIndex.

2. get_cfg_relation.py: This file is used to get CFG graphs of functions using joern tool. The input is output of the first step, and the outputs are stored with folders in cfg_db. 

3. complete_PDG.py: This file is used to get PDG graph of functions. The inputs are files in cfg_db, and the outputs are stored with folders in pdg_db.

4. access_db_operate.py: This file is used to get the call graph of functions. The inputs are files in pdg_db, and the outputs are stored with folders in dict_call2cfgNodeID_funcID.

5. points_get.py: This file is used to get four kinds of SyVCs. The inputs are files in dict_call2cfgNodeID_funcID, and the outputs are four kinds of SyVCs.

6. extract_df.py: This file is used to extract slices. The inputs are files generated by points_get.py, and the outputs are slice files in slices.

7. dealfile.py: This file is used to get the line numbers of vulnerable lines in nvd dataset.The input are source code files(or func files) and diff files.

`` python2 $WORKDIR/source2slice/dealfile.py ``

8. getVulLineForCounting.py: 

This file is used to extract the line numbers of vulnerable lines from SARD_testcaseinfo.xml. 
"000" is the source code file. The output is SARD_testcaseinfo.txt, and then renamed as contain_all.txt (lay in the SARD folder).

```bash 
python $WORKDIR/source2slice/getVulLineForCounting.py $SYSEVR/data/source_data/SARD $SYSEVR/data/source_data/SARD/SARD_testcaseinfo.xml 
mv $SYSEVR/data/source_data/SARD/SARD_testcaseinfo.txt $SYSEVR/data/source_data/SARD/contain_all.txt 
```


Step 9-10 outputs are stored in label_source dir, so you need to create this dir first.

`` mkdir $DATADIR/label_source ``

9. make_label_sard.py: This file is used to get labels of sard slices.

`` python3 $WORKDIR/source2slice/make_label_sard.py ``

10. make_label_nvd.py: This file is used to get labels of nvd slices.

`` python3 $WORKDIR/source2slice/make_label_nvd.py ``

11. data_preprocess.py: This file is used to write the labels to the slice files.  

    => Advantages from this step ??? (may be unnecessary)

`` mkdir $DATADIR/slice_label ``


## Phase 2: Data preprocess

1. create_hash.py: This file is used to get the hash value of slices. The input is slice file generated by extract_df.py,and the output is hashlist of slices.

```bash 
    mkdir $DATADIR/slice_hash 
    python3 $WORKDIR/data_preprocess/create_hash.py
```

2. delete_list.py: This file is used to get index of slices that needed to be delete.  The input is hashlist generated by create_hash.py,and the output is list_delete.

```bash 
    mkdir $DATADIR/list_delete 
    python3 $WORKDIR/data_preprocess/delete_list.py
```

3. process_dataflow_func.py: This file is used to process the slices, including read the pkl file and split codes into corpus. The inputs are the slice file and the label file generated by extract_df.py and make_label_sard.py(make_label_nvd.py), and the output is the corpus file named with testcase id.

```bash 
    mkdir $DATADIR/corpus 
    python3 $WORKDIR/data_preprocess/process_dataflow_func.py
```

4. create_w2vmodel.py: This file is used to train word2vec model. The inputs are corpus files, and the output is the word2vec model.

```bash 
    mkdir $DATADIR/w2v_model
    python3 $WORKDIR/data_preprocess/create_w2vmodel.py
```

5. get_dl_input.py: This file is used to convert tokens of slices in corpus files into vectors by trained word2vec model. The input is the trained word2vec model and corpus files, and the outputs are vector files.

```bash
    # may put trainset and testset in this step into a folder 
    mkdir $DATADIR/vectors $DATADIR/trainset $DATADIR/testset 
    python3 $WORKDIR/data_preprocess/get_dl_input.py
```

6. dealrawdata.py: This file is used to make vectors generated by get_dl_input.py into fixed length. 

```bash 
    mkdir $DATADIR/main_dl_model 
    mkdir $DATADIR/main_dl_model/trainset $DATADIR/main_dl_model/testset 
    python3 $WORKDIR/data_preprocess/dealrawdata.py
```


## Phase 3: Deep learning model

1. bgru.py: This file is used to train BGRU model and get test results. The inputs are vector files generated by dealrawdata.py, and the output is trained model and test results.

```bash 
    mkdir $DATADIR/main_dl_model/result
    python3 $WORKDIR/model/bgru.py
```

2. preprocess_dl_Input_version5.py: This file is used to preprocess data imported into model. It is imported by bgru.py.
